% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\usepackage{textcomp}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={DATA210P HW2 - Bike Sharing (hour.csv): Linear Modeling, Selection, Validation, and Ridge \& Lasso},
  pdfauthor={Joe Nguyen, Haesung Becker, Jared Lyon, Tao Chen},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{DATA210P HW2 - Bike Sharing (hour.csv): Linear Modeling,
Selection, Validation, and Ridge \& Lasso}
\author{Joe Nguyen, Haesung Becker, Jared Lyon, Tao Chen}
\date{}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}

\newpage

\section{Project Overview}\label{project-overview}

\subsection{Libraries \& packages}\label{libraries-packages}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Standard library imports}
\ImportTok{import}\NormalTok{ os}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}

\CommentTok{\# Third{-}party imports}
\ImportTok{from}\NormalTok{ dataclasses }\ImportTok{import}\NormalTok{ dataclass}
\ImportTok{from}\NormalTok{ typing }\ImportTok{import}\NormalTok{ Callable, Dict, Optional, Tuple, List, Any}
\ImportTok{from}\NormalTok{ textwrap }\ImportTok{import}\NormalTok{ dedent}
\ImportTok{import}\NormalTok{ patsy}

\CommentTok{\# Statsmodels imports}
\ImportTok{import}\NormalTok{ statsmodels.api }\ImportTok{as}\NormalTok{ sm}
\ImportTok{import}\NormalTok{ statsmodels.formula.api }\ImportTok{as}\NormalTok{ smf}
\ImportTok{from}\NormalTok{ statsmodels.stats.outliers\_influence }\ImportTok{import}\NormalTok{ variance\_inflation\_factor}
\ImportTok{from}\NormalTok{ statsmodels.graphics.gofplots }\ImportTok{import}\NormalTok{ qqplot}
\ImportTok{from}\NormalTok{ statsmodels.nonparametric.smoothers\_lowess }\ImportTok{import}\NormalTok{ lowess}
\ImportTok{from}\NormalTok{ statsmodels.stats.diagnostic }\ImportTok{import}\NormalTok{ het\_breuschpagan}

\CommentTok{\# Sklearn imports}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ KFold}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_squared\_error}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler}
\ImportTok{from}\NormalTok{ sklearn.pipeline }\ImportTok{import}\NormalTok{ Pipeline}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression, RidgeCV, LassoCV, Ridge, Lasso, lasso\_path}

\CommentTok{\# Set global configurations}
\NormalTok{np.random.seed(}\DecValTok{42}\NormalTok{)}
\NormalTok{pd.set\_option(}\StringTok{"display.max\_columns"}\NormalTok{, }\DecValTok{200}\NormalTok{)}
\NormalTok{pd.set\_option(}\StringTok{"display.width"}\NormalTok{, }\DecValTok{120}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Data Source \& Import:}\label{data-source-import}

Import UCI ML Repo and load dataset (hour.csv). We decided to use the
hour.csv dataset for this homework assignment because it contains a
larger sample size and time-of-day effects for a more robust analysis.

\subsection{Data Dictionary:}\label{data-dictionary}

From data source:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \textbf{Outcome (Response) Variable}: cnt (integer) - count of total
  rental bikes including both casual and registered
\item
  \textbf{Predictor (Feature) Variables}:

  \begin{itemize}
  \tightlist
  \item
    instant (integer) - record index
  \item
    dteday (date) - date
  \item
    season (categorical) - season (1:winter, 2:spring, 3:summer, 4:fall)
  \item
    yr (categorical) - year (0: 2011, 1:2012)
  \item
    mnth (categorical) - month (1 to 12)
  \item
    hr (categorical) - hour (0 to 23)
  \item
    holiday (binary) - whether the day is a holiday or not
  \item
    weekday (categorical) - day of the week
  \item
    workingday (binary) - if day is neither weekend nor holiday is 1,
    otherwise is 0.
  \item
    weathersit (categorical) -

    \begin{itemize}
    \tightlist
    \item
      1: Clear, Few clouds, Partly cloudy
    \item
      2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
    \item
      3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light
      Rain + Scattered clouds
    \item
      4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
    \end{itemize}
  \item
    temp (continuous) - Normalized temperature in Celsius. The values
    are dervied via (t - t\_min)/(t\_max - t\_min), where t\_min=-8,
    t\_max=39 (only in hourly scale)
  \item
    atemp (continuous) - Normalized feeling temperature in Celsius. The
    values are derived via (t - t\_min)/(t\_max - t\_min), where
    t\_min=-16, t\_max=50 (only in hourly scale)
  \item
    hum (continuous) - Normalized humidity. The values are divided to
    100 (max)
  \item
    windspeed (continuous) - Normalized wind speed. The values are
    divided to 67 (max)
  \item
    casual (integer) - count of casual users
  \item
    registered (integer) - count of registered users
  \end{itemize}
\end{enumerate}

Initialize dataframe and perform initial data exploration:

\begin{table}

\caption{\label{tbl-df-shape}Shape and Columns of Data Frame.}

\centering{

\begin{verbatim}
Shape:  (17379, 14)
Columns:
dteday, season, yr, mnth, hr, holiday, weekday, workingday, weathersit, temp,
atemp, hum, windspeed, cnt
\end{verbatim}

}

\end{table}%

\subsection{Initial Exploratory Data Analysis (EDA) with
Visualization}\label{initial-exploratory-data-analysis-eda-with-visualization}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-distributions-output-1.png}}

}

\caption{\label{fig-distributions}Distributions of continuous and
categorical predictors.}

\end{figure}%

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-predictor-vs-cnt-output-1.png}}

}

\caption{\label{fig-predictor-vs-cnt}Relationships between predictors
and hourly bike rentals (cnt).}

\end{figure}%

\section{Linear Model and
Interpretation}\label{linear-model-and-interpretation}

\subsection{Predictor selection and
justification}\label{predictor-selection-and-justification}

After careful consideration from our initial EDA in
Figure~\ref{fig-distributions} and Figure~\ref{fig-predictor-vs-cnt} ,
we've decided to break down the predictor selection into several
categories to fit into the baseline OLS model while excluding the
variables that may lead to redundancy or multicollinearity (\#4):

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{Calendar Variables}: hr, weekday, workingday, holiday

  \begin{itemize}
  \tightlist
  \item
    These variables capture the time-related patterns in bike rentals.
  \end{itemize}
\item
  \textbf{Seasonal Variables}: season, yr

  \begin{itemize}
  \tightlist
  \item
    Seasonal trends and yearly changes can significantly impact bike
    rental behavior.
  \end{itemize}
\item
  \textbf{Weather Variables}: weathersit, temp, hum, windspeed

  \begin{itemize}
  \tightlist
  \item
    Weather conditions significantly influence bike rental behavior and
    demand.
  \end{itemize}
\item
  \textbf{Exclusion of leakage Variables}: dteday, atemp, mnth

  \begin{itemize}
  \tightlist
  \item
    The variable atemp is highly collinear with temp and doesn't much
    predictive power beyond temp. Similar, mnth is highly collinear to
    season and possibly temp.
  \end{itemize}
\end{enumerate}

Additionally, we've also broken them down into specific data types for
clarity:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{Categorical Variables}: season, yr, mnth, hr, holiday,
  weekday, workingday, weathersit

  \begin{itemize}
  \tightlist
  \item
    These variables represent distinct categories or groups.
  \end{itemize}
\item
  \textbf{Continuous Variables}: temp, hum, windspeed

  \begin{itemize}
  \tightlist
  \item
    These variables represent measurable quantities that can take on a
    wide range of values.
  \end{itemize}
\end{enumerate}

\subsection{Fit and show results of OLS
model}\label{fit-and-show-results-of-ols-model}

Table~\ref{tbl-ols-fit} summarizes the baseline model fit statistics
whereas Table~\ref{tbl-ols-coef} provides a coefficient table that's
sorted for lowest p-values.

\begin{table}

\caption{\label{tbl-ols-fit}Baseline OLS model fit statistics.}

\centering{

\begin{verbatim}

--- Statsmodels Summary ---

                            OLS Regression Results                            
==============================================================================
Dep. Variable:                    cnt   R-squared:                       0.682
Model:                            OLS   Adj. R-squared:                  0.681
Method:                 Least Squares   F-statistic:                     929.0
Date:                Fri, 30 Jan 2026   Prob (F-statistic):               0.00
Time:                        01:01:48   Log-Likelihood:            -1.0509e+05
No. Observations:               17379   AIC:                         2.103e+05
Df Residuals:                   17338   BIC:                         2.106e+05
Df Model:                          40                                         
Covariance Type:            nonrobust                                         
======================================================================================
                         coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------------
Intercept            -87.3998      6.258    -13.966      0.000     -99.666     -75.134
C(weathersit)[T.2]   -10.8506      1.928     -5.629      0.000     -14.629      -7.072
C(weathersit)[T.3]   -66.4743      3.248    -20.468      0.000     -72.840     -60.108
C(weathersit)[T.4]   -70.5084     59.255     -1.190      0.234    -186.654      45.637
C(hr)[T.1]           -17.5302      5.382     -3.257      0.001     -28.080      -6.981
C(hr)[T.2]           -26.6938      5.400     -4.943      0.000     -37.278     -16.110
C(hr)[T.3]           -37.3660      5.439     -6.870      0.000     -48.027     -26.705
C(hr)[T.4]           -40.6120      5.442     -7.462      0.000     -51.279     -29.945
C(hr)[T.5]           -23.8207      5.405     -4.407      0.000     -34.415     -13.226
C(hr)[T.6]            35.0407      5.392      6.498      0.000      24.471      45.610
C(hr)[T.7]           170.2513      5.382     31.631      0.000     159.701     180.802
C(hr)[T.8]           310.9038      5.380     57.793      0.000     300.359     321.448
C(hr)[T.9]           163.4304      5.384     30.354      0.000     152.877     173.984
C(hr)[T.10]          108.8632      5.404     20.146      0.000      98.271     119.455
C(hr)[T.11]          134.5404      5.437     24.746      0.000     123.884     145.197
C(hr)[T.12]          174.0695      5.475     31.795      0.000     163.338     184.800
C(hr)[T.13]          169.1885      5.502     30.751      0.000     158.404     179.973
C(hr)[T.14]          153.3648      5.527     27.750      0.000     142.532     164.198
C(hr)[T.15]          162.7674      5.534     29.412      0.000     151.920     173.615
C(hr)[T.16]          224.7619      5.526     40.677      0.000     213.931     235.592
C(hr)[T.17]          378.3610      5.499     68.805      0.000     367.582     389.140
C(hr)[T.18]          346.4628      5.470     63.334      0.000     335.740     357.185
C(hr)[T.19]          237.6637      5.428     43.784      0.000     227.024     248.303
C(hr)[T.20]          157.9849      5.405     29.229      0.000     147.390     168.579
C(hr)[T.21]          108.3892      5.386     20.125      0.000      97.832     118.946
C(hr)[T.22]           71.2341      5.378     13.245      0.000      60.693      81.776
C(hr)[T.23]           32.2676      5.375      6.003      0.000      21.732      42.803
C(weekday)[T.1]    -2.473e+13   9.87e+13     -0.251      0.802   -2.18e+14    1.69e+14
C(weekday)[T.2]    -2.473e+13   9.87e+13     -0.251      0.802   -2.18e+14    1.69e+14
C(weekday)[T.3]    -2.473e+13   9.87e+13     -0.251      0.802   -2.18e+14    1.69e+14
C(weekday)[T.4]    -2.473e+13   9.87e+13     -0.251      0.802   -2.18e+14    1.69e+14
C(weekday)[T.5]    -2.473e+13   9.87e+13     -0.251      0.802   -2.18e+14    1.69e+14
C(weekday)[T.6]       16.1757      2.896      5.586      0.000      10.500      21.852
C(season)[T.2]        42.8726      2.822     15.190      0.000      37.340      48.405
C(season)[T.3]        27.3446      3.648      7.495      0.000      20.194      34.495
C(season)[T.4]        65.7835      2.440     26.966      0.000      61.002      70.565
C(yr)[T.1]            85.4858      1.569     54.498      0.000      82.411      88.560
temp                 244.9964      7.111     34.454      0.000     231.058     258.935
hum                  -68.7842      5.440    -12.645      0.000     -79.447     -58.122
windspeed            -33.9519      6.849     -4.957      0.000     -47.378     -20.526
workingday          2.473e+13   9.87e+13      0.251      0.802   -1.69e+14    2.18e+14
holiday             2.473e+13   9.87e+13      0.251      0.802   -1.69e+14    2.18e+14
==============================================================================
Omnibus:                     1231.774   Durbin-Watson:                   0.498
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2622.294
Skew:                           0.473   Prob(JB):                         0.00
Kurtosis:                       4.652   Cond. No.                     5.74e+14
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 1.54e-25. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
\end{verbatim}

}

\end{table}%

\begin{table}

\caption{\label{tbl-ols-coef}Baseline OLS model fit coefficient table.}

\centering{

\begin{verbatim}

--- Coefficient Table ---

                  term      estimate       std_err        p_value      conf_low     conf_high
0           C(hr)[T.8]  3.109038e+02  5.379574e+00   0.000000e+00  3.003593e+02  3.214483e+02
1          C(hr)[T.18]  3.464628e+02  5.470435e+00   0.000000e+00  3.357402e+02  3.571854e+02
2          C(hr)[T.19]  2.376637e+02  5.428111e+00   0.000000e+00  2.270241e+02  2.483034e+02
3          C(hr)[T.16]  2.247619e+02  5.525528e+00   0.000000e+00  2.139313e+02  2.355925e+02
4          C(hr)[T.17]  3.783610e+02  5.499041e+00   0.000000e+00  3.675824e+02  3.891397e+02
5           C(yr)[T.1]  8.548581e+01  1.568613e+00   0.000000e+00  8.241117e+01  8.856045e+01
6                 temp  2.449964e+02  7.110919e+00  1.136117e-251  2.310583e+02  2.589345e+02
7          C(hr)[T.12]  1.740695e+02  5.474718e+00  1.127114e-215  1.633385e+02  1.848005e+02
8           C(hr)[T.7]  1.702513e+02  5.382477e+00  1.567761e-213  1.597011e+02  1.808015e+02
9          C(hr)[T.13]  1.691885e+02  5.501973e+00  3.130365e-202  1.584041e+02  1.799730e+02
10          C(hr)[T.9]  1.634304e+02  5.384154e+00  3.118441e-197  1.528769e+02  1.739839e+02
11         C(hr)[T.15]  1.627674e+02  5.534014e+00  1.350689e-185  1.519202e+02  1.736147e+02
12         C(hr)[T.20]  1.579849e+02  5.405104e+00  2.277420e-183  1.473904e+02  1.685795e+02
13         C(hr)[T.14]  1.533648e+02  5.526624e+00  7.170303e-166  1.425321e+02  1.641976e+02
14      C(season)[T.4]  6.578348e+01  2.439501e+00  6.302680e-157  6.100181e+01  7.056514e+01
15         C(hr)[T.11]  1.345404e+02  5.436805e+00  6.824192e-133  1.238837e+02  1.451971e+02
16  C(weathersit)[T.3] -6.647434e+01  3.247787e+00   5.115518e-92 -7.284033e+01 -6.010835e+01
17         C(hr)[T.10]  1.088632e+02  5.403667e+00   3.049689e-89  9.827149e+01  1.194550e+02
18         C(hr)[T.21]  1.083892e+02  5.385804e+00   4.632920e-89  9.783245e+01  1.189459e+02
19      C(season)[T.2]  4.287260e+01  2.822412e+00   8.866599e-52  3.734039e+01  4.840481e+01
20           Intercept -8.739983e+01  6.257868e+00   4.333326e-44 -9.966588e+01 -7.513378e+01
21         C(hr)[T.22]  7.123408e+01  5.378033e+00   7.494517e-40  6.069260e+01  8.177557e+01
22                 hum -6.878424e+01  5.439703e+00   1.730957e-36 -7.944660e+01 -5.812187e+01
23      C(season)[T.3]  2.734460e+01  3.648229e+00   6.932776e-14  2.019370e+01  3.449549e+01
24          C(hr)[T.4] -4.061204e+01  5.442219e+00   8.897576e-14 -5.127934e+01 -2.994474e+01
25          C(hr)[T.3] -3.736604e+01  5.438879e+00   6.630284e-12 -4.802679e+01 -2.670529e+01
26          C(hr)[T.6]  3.504071e+01  5.392208e+00   8.339101e-11  2.447144e+01  4.560998e+01
27         C(hr)[T.23]  3.226763e+01  5.374962e+00   1.971727e-09  2.173216e+01  4.280309e+01
28  C(weathersit)[T.2] -1.085061e+01  1.927762e+00   1.844831e-08 -1.462922e+01 -7.072004e+00
29     C(weekday)[T.6]  1.617572e+01  2.895707e+00   2.357089e-08  1.049984e+01  2.185160e+01
30           windspeed -3.395195e+01  6.849485e+00   7.231704e-07 -4.737763e+01 -2.052627e+01
31          C(hr)[T.2] -2.669381e+01  5.399829e+00   7.746697e-07 -3.727802e+01 -1.610960e+01
32          C(hr)[T.5] -2.382073e+01  5.405069e+00   1.053882e-05 -3.441521e+01 -1.322625e+01
33          C(hr)[T.1] -1.753016e+01  5.382121e+00   1.127691e-03 -2.807966e+01 -6.980661e+00
34  C(weathersit)[T.4] -7.050842e+01  5.925504e+01   2.340964e-01 -1.866543e+02  4.563743e+01
35          workingday  2.472602e+13  9.865948e+13   8.021110e-01 -1.686565e+14  2.181085e+14
36     C(weekday)[T.1] -2.472602e+13  9.865948e+13   8.021110e-01 -2.181085e+14  1.686565e+14
37     C(weekday)[T.2] -2.472602e+13  9.865948e+13   8.021110e-01 -2.181085e+14  1.686565e+14
38     C(weekday)[T.3] -2.472602e+13  9.865948e+13   8.021110e-01 -2.181085e+14  1.686565e+14
39     C(weekday)[T.4] -2.472602e+13  9.865948e+13   8.021110e-01 -2.181085e+14  1.686565e+14
40     C(weekday)[T.5] -2.472602e+13  9.865948e+13   8.021110e-01 -2.181085e+14  1.686565e+14
41             holiday  2.472602e+13  9.865948e+13   8.021110e-01 -1.686565e+14  2.181085e+14
\end{verbatim}

}

\end{table}%

\subsection{Interpretation of
coefficients}\label{interpretation-of-coefficients}

Before we begin, all interpretations for the individual variables below
assume that we are \textbf{holding all other variables constant} and
that \textbf{cnt is bike rentals per hour} since we're using the
hour.csv dataset. Out of the 10 variables fitted into the model, we've
selected 7 variables to interpret. Among them are 3 continuous weather
variables with strongly supported effects, 1 encoded categorical
variable to discuss the referencing, 2 calendar variables with unstable
and unsupported effects with potential multicollinearity issues, and the
hour variable enabled by hour.csv to demonstrate its analytical value.
The exclusion of the other 3 variables (weekday, season, and yr) does
not simply that they're not important to our analysis and will be
accounted for in our analysis in later sections.

Our interpretations of the selected coefficients from the baseline OLS
model are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Continuous Weather Variables:

  \begin{itemize}
  \item
    \textbf{Temperature (temp)}: From Table~\ref{tbl-ols-coef}, a
    one-unit increase in normalized temperature (temp) is associated
    with an increase of approximately 233.3 additional bike rentals per
    hour on average. The data strongly rejects the null hypothesis of no
    effect (p \textless{} 0.001) and the 95\% confidence interval (CI):
    {[}215, 252{]}. This suggests that warmer temperatures may
    potentially encourage more bike rentals.
  \item
    \textbf{Humidity (hum)}: From Table~\ref{tbl-ols-coef}, a one-unit
    increase in normalized humidity (hum) is associated with a decrease
    of approximately 81.3 fewer bike rentals per hour on average. The
    data strongly rejects the null hypothesis of no effect (p
    \textless{} 0.001) and the 95\% CI: {[}-70, -92{]}. This indicates
    that higher humidity levels may deter people from renting bikes.
  \item
    \textbf{Windspeed (windspeed)}: From Table~\ref{tbl-ols-coef}, a
    one-unit increase in normalized windspeed (windspeed) is associated
    with a decrease of approximately 36.3 fewer bike rentals per hour on
    average. The data strongly rejects the null hypothesis of no effect
    (p \textless{} 0.001) and the 95\% CI:{[}-23, -50{]}. This suggests
    that windier conditions may discourage bike rentals.
  \end{itemize}
\item
  Categorical Weather Variable:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Weather Situation (weathersit)}: From
    Table~\ref{tbl-ols-coef}, the top 2 weather situations are
    weathersit 3 (T.3) and 2 (T.2) in terms of the strength of the
    statistical evidence supporting an association with hourly bire
    rentals. However, note that the categorical coefficients of T.3 and
    T.2 are relative to the reference weather situation (T.1 or
    clear/few clouds/partly cloudly). T.2 or misty/cloudy weather is
    associated with approximately 10.7 fewer bike rentals per hour on
    average compared to clear weather (T.1). Similarly, T.3 or light
    rain/snowy weather is associated with approximately 65.9 fewer bike
    rentals per hour on average compared to clear weather with a 95\%
    CI:{[}-60, -73{]}. Both effects are supported by strong statistical
    evidence in the data (p \textless{} 0.001). This potentially
    highlights the negative impact of adverse weather conditions on bike
    rental demand.
  \end{itemize}
\item
  Calendar \& Human Behavior Variables:

  \begin{itemize}
  \item
    \textbf{Holiday (holiday)}: From Table~\ref{tbl-ols-coef}, the model
    estimates that holidays are associated with a decrease of
    approximately 2.47e+13 bike rentals compared to to the baseline
    reference of non-holidays which is huge and unstable. In this case,
    the data does not support rejecting the null hypothesis of no effect
    (p = 0.998) with an extremely wide 95\% CI:{[}-1.68e+14, 2.18e+14{]}
    spanning large negative and positive values - indicating that the
    model cannot reliably determine the effect and its direction.
  \item
    \textbf{Working Day (workingday)}: From Table~\ref{tbl-ols-coef},
    the model also estimates that working day is associated with a
    decrease of approximately bike rentals per hour compared to
    non-working days. Similarly to holidays, this estimate is not
    statistically supported (p = 0.978) with an implausibly large
    estimated coefficient of 2.47e+13 and the 95\% CI (-1.68e+14,
    2.18e+14) is extremely wide. This suggests that the model cannot
    reliably determine the effect of working days on bike rentals.
  \end{itemize}

  \textbf{NOTE:} Not only are the holiday and workingday variables not
  statistically supported in this model, their estimated coefficients
  are exactly the same indicating potential multicollinearity between
  these two variables. This is likely due to the fact that non-working
  days are holidays and weekends. We should expect an inverse
  relationship between workingday and holiday.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Hour of the Day (hr)}: From Table~\ref{tbl-ols-coef}, we
    observe that certain hours (e.g., T.8 (8am) and T.16-19 (4-7pm))
    dominates the significance levels (p-values) among the top 5.
    However, we need to be careful in interpreting these coefficients as
    they are relative to the reference hour (midnight-1am). For example,
    T.8 has a coefficient of approximately +310.9 with a 95\% CI:
    {[}300, 321{]}, indicating that bike rentals (cnt) increase
    significantly during this hour compared to midnight, likely due to
    morning commute patterns. In Figure~\ref{fig-hr-effects}, we can see
    the peaks during typical commuting hours (8am and 5pm), suggesting
    that people are more likely to rent bikes during these times for
    commuting purposes.
  \end{itemize}
\end{enumerate}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-hr-effects-output-1.png}}

}

\caption{\label{fig-hr-effects}Estimated hour-of-day effects relative to
the reference hour (midnight/ hr=0).}

\end{figure}%

\subsection{Significance vs.~practical
importance}\label{significance-vs.-practical-importance}

In Table~\ref{tbl-ols-coef} , we observe many predictors in the baseline
OLS model exhibit strong statistical evidence of association with hourly
bike rentals (cnt). However, this does not imply practical importance.
Given the large sample size of the hourly dataset (n = 17,379), even
\textbf{relatively small effects can achieve extremely low p-values}. As
a result, p-values in this context primarily reflect the precision and
consistency of the estimates rather than the magnitude of the effects.

To assess practical importance, we should consider the actual effect
sizes (coefficients) alongside their \textbf{95\% confidence intervals
and their real-world implications}. For example, hour of the day (hr)
and temperature (temp) both show overwhelming statistical support and
large, meaninful effects. During peak commute hours as shown in
Figure~\ref{fig-hr-effects}, bike rentals increase by several hundred
bikes per hour compared to the baseline hour (midnight). Similarly,
temperature also show substantial effects on hourly bike rentals, with
warmer temperatures leading to significant increases in rentals. Other
weather variables like humidity, windspeed, and weather situation also
show meaningful effects, although smaller in magnitude.

In contrast, a select few calendar-based variables carry
\textbf{unstable estimated coefficients and wide confidence intervals}
e.g., workingday with a 95\% CI: {[}-1.69e+14, 2.18e+14{]} suggest that,
under the current model, do not contribute reliably to explaining
variation in hourly bike rentals. Although not discussed in our
interpretation, the categorical variable C(weathersit){[}T.4{]} or
severe weather show a p-value of 0.234 and a 95\% CI: {[}-186.65,
45.64{]} indicating uncertain direction. Conceptually, severe weather
should reduce demand, however, from Figure~\ref{fig-distributions}, we
can observe that weathersit = 4 is very rare in the dataset. Despite
being conceptually practical, this is another example of a variable
being impractical for inference in our model likely due to the data
sparsity.

In summary, this analysis highlights the \textbf{importance of
interpreting statistical significance alongside effect magnitude,
direction, and conceptual relevance}, rather than relying on p-values or
any one metric alone. To improve our analysis and the model's predictive
and inferential performance, further model diagnostics, transformations,
and validation techniques will be considered below.

\section{Transformations and Model
Diagnostics}\label{transformations-and-model-diagnostics}

\subsection{Baseline OLS Model
diagnostics}\label{baseline-ols-model-diagnostics}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-residual-diagnostics-baseline-ols-output-1.png}}

}

\caption{\label{fig-residual-diagnostics-baseline-ols}Residual
diagnostics for the baseline OLS model.}

\end{figure}%

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-qq-plot-baseline-ols-output-1.png}}

}

\caption{\label{fig-qq-plot-baseline-ols}Normal Q--Q plot of studentized
residuals from the baseline OLS model.}

\end{figure}%

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-scale-location-baseline-ols-output-1.png}}

}

\caption{\label{fig-scale-location-baseline-ols}Scale--location plot of
studentized residuals for the baseline OLS model.}

\end{figure}%

\begin{table}

\caption{\label{tbl-breusch-pagan-baseline-ols}Breusch--Pagan test for
heteroskedasticity in the baseline OLS model.}

\centering{

\begin{verbatim}
LM Statistic    5504.267929
LM p-value         0.000000
F Statistic      200.916107
F p-value          0.000000
dtype: float64
\end{verbatim}

}

\end{table}%

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-residuals-vs-leverage-baseline-ols-output-1.png}}

}

\caption{\label{fig-residuals-vs-leverage-baseline-ols}Studentized
residuals versus leverage for the baseline OLS model.}

\end{figure}%

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-cooks-distance-baseline-ols-output-1.png}}

}

\caption{\label{fig-cooks-distance-baseline-ols}Cook's distance for the
baseline OLS model.}

\end{figure}%

From our diagnostic plots above, we can visually and numerically
evaluate our baseline OLS model's assumptions as follows:

With respect to linearity in
Figure~\ref{fig-residual-diagnostics-baseline-ols}, there is a U-shaped
curvature in the LOESS line in the Residuals vs.~Fitted plot indicating
\textbf{systematic departures from linearity} in the overall mean
function indicating global nonlinearity at low and high fitted values.
This could lead to prediction bias, misleading coefficients, and
unreliable statistical inference. As a result, these issues motivate the
consideration of transformations or alternative modeling approaches that
better capture nonlinear relationships.

In further analysis, we plotted the 3 continuous predictors (temp, hum,
and windspeed) against their respective partial residuals in
Figure~\ref{fig-residual-diagnostics-baseline-ols}. When plotted using
the partial residuals vs temperature (temp), the LOESS line has mild
slope changes at low and high temperatures, suggesting the
\textbf{linearity assumption is not violated}. Similarly for humidity
(hum) and windspeed, we observe no significant curvatures that would
hint at nonlinearity other than mild curvatures at the tails.

In Figure~\ref{fig-qq-plot-baseline-ols} for checking normality, using
the studentized residuals, the normal Q-Q plot shows that while most of
the distribution is approximately normal, there is clear departures
occuring at the upper tail. This suggests \textbf{heavy-tailed errors
and a violation of the normality assumption} and that our p-values and
CIs may not be reliable for inference. Once again, another motivation
for transformations or alternative modeling methods.

In Figure~\ref{fig-scale-location-baseline-ols} for validating
homoskedasticity, we observe a funnel shape in which studentized
residual spread as fitted values increase. The lower tail is sharply
bounded whereas the upper tail grows. This observation hints at
\textbf{heteroskedasticity and our assumption of constant residual
variance is violated}. This motivates further variance-stabilizing
transformations. \textbf{Supported by the Breusch-Pagan test} in
Table~\ref{tbl-breusch-pagan-baseline-ols} with a p-value \textless{}
0.05 i.e., there is strong support in rejecting the null hypothesis of
homoskedasticity - there's evidence of heteroskedasticity.

In Figure~\ref{fig-residuals-vs-leverage-baseline-ols} for confirming
leverage and influence in observations, there are a small number of
observations with high leverage and moderate studentized residuals
whereas the rest of the dataset remain small and well below thresholds
associated with undue influence. Consistent with
Figure~\ref{fig-cooks-distance-baseline-ols}, only two observations with
high leverage and moderate residual show up as high influence on Cook's
D near 0.02 and 0.025 which are very minimal in comparison to the 0.5
threshold that would be of concern. As a result, there's \textbf{no
evidence that the fitted model is overly sensitive to any small number
of influential observations}.

\subsection{Transformations}\label{transformations}

To address the assumption violations above, we chose to apply a
\textbf{log transformation to the response variable (cnt)}. In our early
EDA, we observed that cnt has a heavily skewed right-tail as observed in
Figure~\ref{fig-distributions}. Additionally, this transformation is
also motivated by the presence of heteroskedasticity, heavy-tailed
residuals, and global nonlinearity in the baseline OLS model. The log
transformation should stabilize the variance and reduce the influence of
extreme outliers. A constant of one is added to accomodate for zero
counts.

\textbf{Important note:} the log transformation changes our
interpretation of estimated coefficients to \textbf{approximate percent
change in hourly bike rentals}.

\subsection{Refit model and compare}\label{refit-model-and-compare}

\begin{longtable}[]{@{}lllll@{}}

\caption{\label{tbl-ols-baseline-vs-log-metrics}In-sample model
comparison metrics for the baseline and log-transformed OLS models.}

\tabularnewline

\toprule\noalign{}
& Model & Adj\_R\^{}2 & AIC & BIC \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & Baseline OLS & 0.681128 & 210260.540315 & 210578.824047 \\
1 & Log-Transformed OLS & 0.825787 & 31126.973333 & 31445.257065 \\

\end{longtable}

Our model comparison includes in-sample (data seen) and out-of-sample
(data unseen) metrics. In-sample statistics such as adjusted R\^{}2,
AIC, and BIC provide descriptive measures of fit for their respective
response scales (baseline vs.~log-transformed OLS). In
Table~\ref{tbl-ols-baseline-vs-log-metrics}, the log-transformed model
has a substantially higher adjusted R\^{}2 indicating \textbf{improved
explanatory power} i.e., closer fit to the observed data, but not a
definitive improvement in fit from the original scale.

Whereas the large decrease in AIC and BIC values, this tells us that the
log-transformed model improved in-sample likelihood fit i.e., the
\textbf{model's assumptions about noise and stability match the data
better} - likely due to improved normality and stabilizing residual
variance.

In contrast, the out-of-sample evaluation using k-fold cross-validated
RMSE is \textbf{deferred to part IV} and will serve as the primary
criterion for predictive performance and model selection.

\section{Collinearity Assessment}\label{collinearity-assessment}

\subsection{Correlation matrix with numeric
predictors}\label{correlation-matrix-with-numeric-predictors}

\begin{table}

\caption{\label{tbl-corr-numeric-predictors}Correlation matrix for
numeric predictors.}

\centering{

\begin{verbatim}
               temp       hum  windspeed
temp       1.000000 -0.069881  -0.023125
hum       -0.069881  1.000000  -0.290105
windspeed -0.023125 -0.290105   1.000000
\end{verbatim}

}

\end{table}%

\subsection{VIF analysis}\label{vif-analysis}

\begin{longtable}[]{@{}lll@{}}

\caption{\label{tbl-vif-log-ols}Top 15 Variance Inflation Factors (VIF)
for predictors in the log-transformed OLS model.}

\tabularnewline

\toprule\noalign{}
& variable & VIF \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
30 & C(weekday){[}T.4{]} & inf \\
31 & C(weekday){[}T.5{]} & inf \\
27 & C(weekday){[}T.1{]} & inf \\
29 & C(weekday){[}T.3{]} & inf \\
28 & C(weekday){[}T.2{]} & inf \\
41 & holiday & inf \\
40 & workingday & inf \\
34 & C(season){[}T.3{]} & 4.228093 \\
37 & temp & 3.105377 \\
33 & C(season){[}T.2{]} & 2.498219 \\
18 & C(hr){[}T.15{]} & 2.038059 \\
19 & C(hr){[}T.16{]} & 2.033286 \\
17 & C(hr){[}T.14{]} & 2.031779 \\
20 & C(hr){[}T.17{]} & 2.014788 \\
16 & C(hr){[}T.13{]} & 2.014241 \\

\end{longtable}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-vif-log-ols-top10-finite-output-1.png}}

}

\caption{\label{fig-vif-log-ols-top10-finite}Top finite variance
inflation factors (VIF) for predictors in the log-transformed OLS model.
Predictors with infinite VIF arise from exact multicollinearity among
dummy-encoded categorical variables and are omitted from this plot.}

\end{figure}%

\subsection{Discussion of collinearity
effects}\label{discussion-of-collinearity-effects}

In Table~\ref{tbl-corr-numeric-predictors}, the correlation matrix among
numerically continous predictors (temp, hum, and windspeed) indicates no
strong pairwise linear relationships. While humidity and windspeed show
a modest negative association (\textasciitilde{} -0.29), none of the
absolute correlation values approach problematic levels for collinearity
(e.g., \textbar p\textbar{} \textgreater{} 0.7).

In Table~\ref{tbl-vif-log-ols}, VIF values are reported in descending
order. Several categorical and binary variables with \textbf{inf VIF
values indicate perfect multicollinearity or exact linear combinations}
of one another once encoding is applied e.g., workingday is defined by
weekday and holiday with overlapping calendar information. The remaining
variables as shown in Figure~\ref{fig-vif-log-ols-top10-finite} are
considered in moderate VIF values (\textasciitilde{} 2-5) or low values
(\textless= 1) without raising concerns for multicollinearity. However,
it's important to highlight the potential collinearity between season
and temp at the top of the list. Since temperature changes with the
season, these two variables are likely to provide overlapping
information. Something we will explore more in section IV.3 for model
selection.

As a result, calendar-related categorical and binary variables with
perfect multicollinearity, due to overlapping definitions, can lead to
\textbf{variance inflation of standard errors and complicated
coefficient interpretations} as discusses in section I.4. Although
collinearity complicates interpretability and statistical inference, it
\textbf{does not necessarily harm predictive performance} because
predictions depend on combined linear predictors with redundant
variables that may jointly capture meaningful structure in the data.

Several strategies can be used to address collinearity, depending on
\textbf{whether the primary goal is interpretability or predictive
performance}. Common strategies include: 1) Remove redundant predictors:
remove workingday and keep weekday \& holiday to improve interpretation
2) Redesign variable encodings: represent calendar effects solely with
weekday indicators 3) Combine correlated variables: temp\_index = (temp
+ atemp) / 2 4) Apply regularization techniques: Ridge or Lasso to
preserve predictive performance

\section{Model Selection and
Validation}\label{model-selection-and-validation}

\subsection{Cross-validation \& Utilities
Setup}\label{cross-validation-utilities-setup}

Here, we define 3 functions to leverage in model selection and
validation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  cv\_metric\_formula: to calculate MSE or RMSE given the parameters
  (nested inside stepwise\_selection\_cv)
\item
  stepwise\_selection\_cv: to conduct stepwise selection
  cross-validation based on RMSE or MSE as primary criterion
\item
  stepwise\_selection\_aic\_bic: to conduct stepwise selection
  cross-validation based on AIC or BIC as primary criterion
\end{enumerate}

\subsection{Baseline model CV RMSE
results}\label{baseline-model-cv-rmse-results}

\begin{table}

\caption{\label{tbl-baseline-model-rmse}RMSE mean and standard deviation
for baseline log cnt OLS model.}

\centering{

\begin{verbatim}
Baseline log cnt OLS model RMSE: 0.5925789002917841 SD: 0.006101317168347299
\end{verbatim}

}

\end{table}%

\subsection{Stepwise selection}\label{stepwise-selection}

\begin{table}

\caption{\label{tbl-model-selection-rmse}Stepwise five-fold
cross-validation with Root Mean Squared Error (RMSE) for candidate
log-transformed OLS models.}

\centering{

\begin{verbatim}
Start: RMSE=1.417834 (sd=0.014040) | log_cnt ~ 1
ADD  C(hr)                -> RMSE=0.774129 (sd=0.003304)
ADD  temp                 -> RMSE=0.683957 (sd=0.005311)
ADD  C(yr)                -> RMSE=0.643233 (sd=0.005164)
ADD  C(weathersit)        -> RMSE=0.621278 (sd=0.005742)
ADD  C(season)            -> RMSE=0.597358 (sd=0.005879)
ADD  C(weekday)           -> RMSE=0.594297 (sd=0.006039)
ADD  hum                  -> RMSE=0.593732 (sd=0.005565)
ADD  windspeed            -> RMSE=0.593172 (sd=0.005992)
ADD  workingday           -> RMSE=0.592592 (sd=0.006158)

Best formula: log_cnt ~ C(hr) + temp + C(yr) + C(weathersit) + C(season) + C(weekday) + hum + windspeed + workingday
RMSE: 0.5925916101069012 SD: 0.006158486580407104
\end{verbatim}

}

\end{table}%

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-stepwise-cv-rmse-step-output-1.png}}

}

\caption{\label{fig-stepwise-cv-rmse-step}Stepwise five-fold
cross-validation path showing mean RMSE (±1 SD) versus step index for
log-transformed OLS models.}

\end{figure}%

\begin{table}

\caption{\label{tbl-model-selection-mse}Stepwise five-fold
cross-validation with Mean Squared Error (MSE) for candidate
log-transformed OLS models.}

\centering{

\begin{verbatim}
Start: MSE=2.010411 (sd=0.039871) | log_cnt ~ 1
ADD  C(hr)                -> MSE=0.599285 (sd=0.005114)
ADD  temp                 -> MSE=0.467819 (sd=0.007279)
ADD  C(yr)                -> MSE=0.413770 (sd=0.006639)
ADD  C(weathersit)        -> MSE=0.386012 (sd=0.007119)
ADD  C(season)            -> MSE=0.356864 (sd=0.007008)
ADD  C(weekday)           -> MSE=0.353218 (sd=0.007165)
ADD  hum                  -> MSE=0.352542 (sd=0.006600)
ADD  windspeed            -> MSE=0.351882 (sd=0.007098)
ADD  workingday           -> MSE=0.351195 (sd=0.007287)

Best formula: log_cnt ~ C(hr) + temp + C(yr) + C(weathersit) + C(season) + C(weekday) + hum + windspeed + workingday
MSE: 0.3511951579346585 SD: 0.007286777545519475
\end{verbatim}

}

\end{table}%

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-stepwise-cv-mse-step-output-1.png}}

}

\caption{\label{fig-stepwise-cv-mse-step}Stepwise five-fold
cross-validation path showing mean MSE (±1 SD) versus step index for
log-transformed OLS models.}

\end{figure}%

\begin{table}

\caption{\label{tbl-model-selection-aic-bic}Stepwise five-fold
cross-validation with BIC for candidate log-transformed OLS models.}

\centering{

\begin{verbatim}
Start: BIC=61464.164802 | log_cnt ~ 1
ADD  C(hr)                -> BIC=40593.280281
ADD  temp                 -> BIC=36299.400639
ADD  C(yr)                -> BIC=34178.461775
ADD  C(weathersit)        -> BIC=32991.065329
ADD  C(season)            -> BIC=31652.933488
ADD  C(weekday)           -> BIC=31523.283631
ADD  hum                  -> BIC=31495.422792
ADD  windspeed            -> BIC=31469.720397
ADD  workingday           -> BIC=31444.832698

Best formula: log_cnt ~ C(hr) + temp + C(yr) + C(weathersit) + C(season) + C(weekday) + hum + windspeed + workingday

Score: 31444.83269754891
\end{verbatim}

}

\end{table}%

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-stepwise-bic-step-output-1.png}}

}

\caption{\label{fig-stepwise-bic-step}Stepwise selection path showing
Bayesian Information Criterion (BIC) versus step index for
log-transformed OLS models.}

\end{figure}%

Our model selection was performed using \textbf{stepwise 5-fold
cross-validation (CV) with RMSE} as the selection criterion as mentioned
in section III.3 and shown in Table~\ref{tbl-model-selection-rmse} and
Figure~\ref{fig-stepwise-cv-rmse-step}. Given the size of the dataset,
5-fold CV was adequate for our analysis within considerations for time
and resources. The motivation for using RMSE is that CV RMSE directly
\textbf{evaluates out-of-sample predictive performance} compared to
AIC/BIC and better for interpretation than MSE with the same
penalization for outliers. Unlike likelihood-based criteria (AIC/BIC),
CV RMSE makes minimal assumptions and provides a robust assessment of
\textbf{generalization error} especially in the presence of correlated
predictors and large sample sizes.

In the stepwise selection logs for RMSE in
Table~\ref{tbl-model-selection-rmse} and depicted in
Figure~\ref{fig-stepwise-cv-rmse-step}, significant reductions in error
were observed when adding core time and weather-related predictors
(C(hr), temp, C(yr), C(weathersit), and C(season)). However, beyond
these core predictors, additional predictors produced only
\textbf{marginal improvements in RMSE}, with reductions within one
standard deviation of the CV estimates. This flattening or convergence
of the RMSE curve (\textasciitilde0.592-0.597) shows a
\textbf{diminishing predictive returns} and led us to a selection of a
reduced model that balanced \textbf{predictive accuracy with minimal
complexity for interpretation}.

Among these, one key observation was the \textbf{complete exclusion of
the holiday predictor} while retaining both weekday and workingday.
These specifications could highlight the value of preserving some
relevant calendar and weather structures while eliminating weak or
redundant predictors like hum, windspeed, and workingday, without
sacrificing out-of-sample predictive performance.

Lastly, we also experimented with stepwise 5-fold cross-validation with
MSE and BIC as shown in Table~\ref{tbl-model-selection-mse} and
Table~\ref{tbl-model-selection-aic-bic}. Overall, both experiments (MSE
and BIC) were \textbf{consistent with the same observations} as
mentioned above. Criterion scores begin to converge (MSE:
\textasciitilde0.351-0.353 / BIC: \textasciitilde31,444-31,523) after
adding C(hr), temp, C(yr), C(weathersit), and C(season) with minimal
additional benefit from including remaining variables.

\subsection{Compare selected models
vs.~baseline}\label{compare-selected-models-vs.-baseline}

\begin{longtable}[]{@{}lllll@{}}

\caption{\label{tbl-model-selection-analysis-in-sample}In-sample
evaluation for Baseline vs.~Selected OLS model for log cnt using
Adjusted R\^{}2, AIC, and BIC.}

\tabularnewline

\toprule\noalign{}
& Model & Adj\_R\^{}2 & AIC & BIC \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & Baseline log OLS & 0.825786 & 31127.080796 & 31445.364528 \\
1 & Selected model & 0.822891 & 31404.516917 & 31652.933488 \\

\end{longtable}

\begin{table}

\caption{\label{tbl-model-selection-analysis-out-of-sample}CV RMSE
evaluation for selected OLS model for log cnt.}

\centering{

\begin{verbatim}
Start: RMSE=1.417834 (sd=0.014040) | log_cnt ~ 1
ADD  C(hr)                -> RMSE=0.774129 (sd=0.003304)
ADD  temp                 -> RMSE=0.683957 (sd=0.005311)
ADD  C(yr)                -> RMSE=0.643233 (sd=0.005164)
ADD  C(weathersit)        -> RMSE=0.621278 (sd=0.005742)
ADD  C(season)            -> RMSE=0.597358 (sd=0.005879)
RMSE: 0.5973581846445841 SD: 0.0058789993280516616
\end{verbatim}

}

\end{table}%

The reduced/selected model, where the variables with negligible
out-of-sample CV RMSE improvements were excluded, was compared to the
baseline log-transformed OLS model using both in-sample metrics
(adjusted R\^{}2, AIC, and BIC) and out-of-sample CV RMSE. The in-sample
comparison is shown in
Table~\ref{tbl-model-selection-analysis-in-sample}, whereas the CV RMSE
comparison is presented in Table~\ref{tbl-model-selection-rmse} and
Table~\ref{tbl-model-selection-analysis-out-of-sample}, as discussed in
section IV.3.

The adjusted R\^{}2 of the selected model slightly lower than the
baseline model, reflecting the intentional removal of predictors.
However, the difference is small, showing that the \textbf{reduced model
retains most of the explanatory power with fewer predictors}. This is
expected since adjusted R\^{}2 penalizes complexity minimally and, as a
result, not designed as a primary model selection criterion.

Both the AIC and BIC increased for the reduced model. This indicates a
preference for the larger baseline model under likelihood-based
criteria. While this was initially surprising since we expected BIC to
penalize complexity heavily, it is likely driven by the large sample
size because \textbf{small improvements in log-likelihood over many
observations can outweigh the complexity penalty} especially for
predictors with subtle but detectable effects.

Most importantly, this does not contradict our CV RMSE findings. AIC and
BIC evaluate goodness-of-fit through the lens of likelihood i.e.,
goodness-of-fit to observed data, whereas CV RMSE evaluates predictive
stability on unseen data. Our findings highlight a \textbf{modeling
trade-off} between likelihood criteria that favor richer models in large
samples, while CV emphasizes gains in predictive performance. In our
case, our decision for the reduced model prioritizes generalization
performance over marginal in-sample improvements and avoids
over-interpreting weak effects or collinearity among time and
calendar-related predictors.

In the next section, we'll explore regularization techniques that may
allow all predictors to remain in the baseline model while shrinking
correlated coefficients toward each other or possibly shrinking the same
weaker variables that we removed to zero as part of feature selection.

\section{Ridge and Lasso Regression}\label{ridge-and-lasso-regression}

\subsection{Ridge Regression}\label{ridge-regression}

\begin{table}

\caption{\label{tbl-ridge-cv-results}Ridge regression: cross-validated
selection of the regularization parameter (alpha) and predictive
performance.}

\centering{

\begin{verbatim}
Ridge Regression with 5-fold CV selection
Best ridge alpha: 54.62277217684348
\end{verbatim}

}

\end{table}%

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-ridge-coefficient-path-output-1.png}}

}

\caption{\label{fig-ridge-coefficient-path}Ridge regression coefficient
paths showing standardized coefficients as a function of log10(λ), with
the dashed line indicating the cross-validated optimal regularization
parameter.}

\end{figure}%

\subsection{Lasso regression}\label{lasso-regression}

\begin{table}

\caption{\label{tbl-lasso-cv-results}Lasso regression: cross-validated
selection of the regularization parameter (alpha) and predictive
performance.}

\centering{

\begin{verbatim}
Lasso Regression with 5-fold CV selection
alpha_min: 0.0009326033468832199
alpha_1se: 0.010476157527896652
min mean MSE: 0.3698843993068294
1-SE threshold: 0.3898582268015863
\end{verbatim}

}

\end{table}%

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-lasso-coefficient-path-output-1.png}}

}

\caption{\label{fig-lasso-coefficient-path}Lasso coefficient paths
showing standardized coefficients as a function of log10(alpha), with
dashed lines indicating alpha\_min (minimum CV error) and alpha\_1se
(one-standard-error rule).}

\end{figure}%

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{02_bike_sharing_files/figure-pdf/fig-lasso-cv-curve-output-1.png}}

}

\caption{\label{fig-lasso-cv-curve}Lasso cross-validation curve showing
mean CV MSE (±1 SE) as a function of log10(alpha), with dashed lines
indicating alpha\_min and alpha\_1se.}

\end{figure}%

\begin{longtable}[]{@{}lllll@{}}

\caption{\label{tbl-lasso-selection-comparison}Comparison of predictors
selected by lasso regression under the minimum cross-validated error
rule (alpha\_min) and the one-standard-error rule (alpha\_1se). For each
model, retained predictors and their corresponding coefficients are
shown; blank cells indicate predictors not selected under the given
regularization level.}

\tabularnewline

\toprule\noalign{}
& Predictor (alpha\_min) & Coefficient (alpha\_min) & Predictor
(alpha\_1se) & Coefficient (alpha\_1se) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & C(hr){[}T.17{]} & 0.406811 & C(hr){[}T.4{]} & -0.455284 \\
1 & C(hr){[}T.18{]} & 0.391467 & C(hr){[}T.3{]} & -0.402966 \\
2 & C(hr){[}T.4{]} & -0.374228 & temp & 0.356028 \\
3 & C(hr){[}T.8{]} & 0.358817 & C(hr){[}T.2{]} & -0.306849 \\
4 & C(hr){[}T.19{]} & 0.335117 & C(hr){[}T.17{]} & 0.291569 \\
5 & C(hr){[}T.16{]} & 0.327749 & C(hr){[}T.18{]} & 0.277849 \\
6 & C(hr){[}T.3{]} & -0.321401 & C(hr){[}T.5{]} & -0.269552 \\
7 & temp & 0.300879 & C(hr){[}T.8{]} & 0.254554 \\
8 & C(hr){[}T.9{]} & 0.294801 & C(hr){[}T.19{]} & 0.223850 \\
9 & C(hr){[}T.12{]} & 0.287836 & C(hr){[}T.1{]} & -0.213825 \\
10 & C(hr){[}T.13{]} & 0.282424 & C(yr){[}T.1{]} & 0.213509 \\
11 & C(hr){[}T.20{]} & 0.278079 & C(hr){[}T.16{]} & 0.211511 \\
12 & C(hr){[}T.15{]} & 0.276665 & C(hr){[}T.9{]} & 0.187935 \\
13 & C(hr){[}T.14{]} & 0.266417 & C(season){[}T.4{]} & 0.175481 \\
14 & C(hr){[}T.11{]} & 0.251702 & C(hr){[}T.12{]} & 0.174027 \\
15 & C(hr){[}T.7{]} & 0.234265 & C(hr){[}T.20{]} & 0.168766 \\
16 & C(hr){[}T.10{]} & 0.229266 & C(hr){[}T.13{]} & 0.167215 \\
17 & C(hr){[}T.21{]} & 0.228082 & C(hr){[}T.15{]} & 0.160057 \\
18 & C(yr){[}T.1{]} & 0.227399 & C(hr){[}T.14{]} & 0.150109 \\
19 & C(hr){[}T.2{]} & -0.223349 & C(hr){[}T.11{]} & 0.140110 \\
20 & C(season){[}T.4{]} & 0.214505 & C(weathersit){[}T.3{]} &
-0.133944 \\
21 & C(hr){[}T.5{]} & -0.187458 & C(hr){[}T.7{]} & 0.131777 \\
22 & C(hr){[}T.22{]} & 0.178496 & C(hr){[}T.21{]} & 0.120582 \\
23 & C(weathersit){[}T.3{]} & -0.154328 & C(hr){[}T.10{]} & 0.119946 \\
24 & C(season){[}T.2{]} & 0.145269 & C(season){[}T.2{]} & 0.087729 \\
25 & C(hr){[}T.1{]} & -0.128913 & C(hr){[}T.22{]} & 0.072219 \\
26 & C(season){[}T.3{]} & 0.112694 & hum & -0.067306 \\
27 & C(hr){[}T.23{]} & 0.100751 & C(season){[}T.3{]} & 0.038873 \\
28 & hum & -0.046635 & C(hr){[}T.6{]} & -0.038587 \\
29 & C(hr){[}T.6{]} & 0.042595 & C(weekday){[}T.5{]} & 0.029429 \\
30 & C(weekday){[}T.5{]} & 0.037252 & C(weekday){[}T.6{]} & 0.026240 \\
31 & C(weekday){[}T.6{]} & 0.033168 & holiday & -0.020356 \\
32 & windspeed & -0.027126 & windspeed & -0.010901 \\
33 & holiday & -0.026923 & C(weekday){[}T.2{]} & -0.006324 \\
34 & C(weathersit){[}T.2{]} & -0.018609 & C(weekday){[}T.1{]} &
-0.002233 \\
35 & C(weekday){[}T.2{]} & -0.014622 & C(weekday){[}T.3{]} &
-0.001446 \\
36 & C(weekday){[}T.3{]} & -0.009896 & C(weathersit){[}T.2{]} &
-0.000602 \\
37 & C(weekday){[}T.1{]} & -0.009535 & NaN & NaN \\
38 & C(weekday){[}T.4{]} & 0.005620 & NaN & NaN \\
39 & C(weathersit){[}T.4{]} & -0.001120 & NaN & NaN \\

\end{longtable}

\begin{longtable}[]{@{}lll@{}}

\caption{\label{tbl-lasso-zeroed-out-comparison}Predictors shrunk to
zero by lasso under the minimum cross-validated error rule (alpha\_min)
and the one-standard-error rule (alpha\_1se).}

\tabularnewline

\toprule\noalign{}
& Zeroed out (alpha\_min) & Zeroed out (alpha\_1se) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & workingday & C(hr){[}T.23{]} \\
1 & & C(weathersit){[}T.4{]} \\
2 & & C(weekday){[}T.4{]} \\
3 & & workingday \\

\end{longtable}

\subsection{Selected OLS vs.~Ridge vs.~Lasso
comparison}\label{selected-ols-vs.-ridge-vs.-lasso-comparison}

\begin{longtable}[]{@{}lllll@{}}

\caption{\label{tbl-model-comparison-regularization}Comparison of
selected OLS, ridge, and lasso models based on cross-validated
predictive performance and regularization characteristics, including the
one-standard-error (1-SE) lasso solution.}

\tabularnewline

\toprule\noalign{}
& Model & Selected alpha & CV RMSE (mean) & CV RMSE (SD) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & Selected OLS & --- & 0.597358 & 0.005879 \\
1 & Ridge & 54.622772 & 0.592777 & 0.006014 \\
2 & Lasso (alpha\_min) & 0.000933 & 0.592737 & 0.005986 \\
3 & Lasso (alpha\_1se) & 0.010476 & 0.607567 & 0.005406 \\

\end{longtable}

In this section (V), we explore Ridge and Lasso regularization
techniques intended to \textbf{improve generalization in the presence of
predictors with strong collinearity}. By penalizing large coefficients
and reducing variance, regularization stabilizes estimates of correlated
predictors (e.g., many encoded hour (hr) or categorical predictors).
Additionally, it provides a bias-variance tradeoff that could help
ensure a balance in-sample fit to reliable out-of-sample predictive
performance. Both regularization techniques were \textbf{performed on
the baseline log transformed OLS model} before reduction in predictors.

In Table~\ref{tbl-ridge-cv-results}, we leveraged a 5-fold CV to select
a near optimal regularization parameter (alpha = 54.62) while taking
advantage of the Ridge squared coefficient penalty that shrinks them
toward zero while \textbf{sharing weights across correlated predictors
without removing any} as shown in
Figure~\ref{fig-ridge-coefficient-path}.

Similarly, in Table~\ref{tbl-lasso-cv-results}, two regularization
parameters (alpha\_min and alpha\_1se) were derived using 5-fold
cross-validation with MSE as the natural loss function. The alpha\_min
value selects the model that minimizes mean CV MSE, thereby
\textbf{maximizing predictive accuracy}. In contrast, alpha\_1se,
defined as the largest regularization parameter whose mean CV MSE lies
within one standard error of the minimum, is designed to \textbf{favor
simpler and more stable models without a meaningful loss in predictive
accuracy}.

As shown in Figure~\ref{fig-lasso-coefficient-path} and
Figure~\ref{fig-lasso-cv-curve}, increasing the regularization strength
leads to \textbf{progressive coefficient shrinkage and variable
elimination}. The alpha\_min model retains more predictors, with only
one coefficient (workingday) shrunk exactly to zero, whereas the
alpha\_1se model applies stronger regularization and collapses four
predictors to zero, as confirmed in
Table~\ref{tbl-lasso-selection-comparison} and
Table~\ref{tbl-lasso-zeroed-out-comparison}. Together, these visuals
illustrate the impact of increasing regularization on both predictive
error (MSE) and coefficient sparsity.

Overall, Table~\ref{tbl-model-comparison-regularization} supports the
following conclusions: 1) In comparison to our reduced/selected
log-transformed OLS model (Selected OLS in
Figure~\ref{fig-lasso-coefficient-path}), we observe \textbf{marginal
improvements of approximately 0.005 RMSE} in Ridge and Lasso
(alpha\_min). Differences in RMSE are small relative to their SDs with
\textbf{no meaningful separation in predictive accuracy}. Regularization
helps modestly, but predictive performance is not the main
differentiator 2) However, in terms of interpretability, Lasso
(alpha\_1se) removed four predictors and produced a substantially
\textbf{sparser and more interpretable model}, at the cost of a small
RMSE increase 3) Lastly, in terms of regularization effects on
collinearity, Ridge effectively shrank correlated coefficients together
(e.g, the many correlated categorical predictors like C(hr))
\textbf{without removing any predictors}

\section{Conclusion}\label{conclusion}

\subsection{Summary of findings}\label{summary-of-findings}

This project analyzed hourly bike-sharing demand using a combination of
exploratory analysis, classical regression, and regularized models to
\textbf{balance predictive accuracy, interpretability, and stability}.
The goal was to identify key drivers for hourly bike rentals across a
high-dimensional feature space dominated by categorical temporal and
weather predictors.

Across all models, the time of day or hour emerged the strongest
predictor of bike rental demand, reflecting clear commuting and leisure
patterns. Temperature showed a significant positive association, while
weather conditions and seasonal effects contributed secondary but
meaningful adjustments. With time of day effects include, several
calendar variables like workingday, weekday, or holiday provided limited
additional explanatory power, likely due to redundancy among correlated
predictors.

From a predictive accuracy standpoint, the reduced OLS, Ridge, and Lasso
(alpha\_min) achieved nearly identical CV RMSE. This highlights the fact
that prediction alone does not fully justify regularization in this
case. However, the \textbf{primary value of regularization was seen in
stability and interpretability}. By applying the 1-SE rule to Lasso, our
feature selection produced a substantially sparser model with little
loss in RMSE, while improving interpretability.

In conclusion, the results show that bike-sharing demand is strongly
associated to temporal structure and weather-related factors.




\end{document}
