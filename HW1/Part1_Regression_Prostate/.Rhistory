geom_point(alpha = 0.7) +
geom_abline(
data = ref_line,
aes(slope = slope, intercept = intercept, color = label),
linewidth = 1
) +
scale_color_manual(values = c("Perfect Prediction" = "black")) +
labs(
title = "Multiple Linear Regression (MLR): Predicted vs Actual lpsa (Test Set)",
x = "Actual lpsa (Log PSA)",
y = "Predicted lpsa (Log PSA)",
color = "Legend"
)
# MLR Conclusion: The MLR model using all predictors in the test set shows worse performance compared to the SLR.
# The RMSE increased from 0.6926 to 0.7220, and R-squared decreased from 0.5438 to 0.5052.
# Based on these metrics, the SLR explains more variance in the outcome than MLR and has lower prediction error.
# This suggests that adding more predictors introduced noise and overfitting rather than improving generalization.
# Assumptions of Linear Regression: Normality, homoskedasticity, independence
# Create diagnostic plots using base R plotting
par(mfrow = c(2, 2)) # set plotting layout to 2x2
plot(slr_fit)  # residuals vs fitted, QQ plot, scale-location, leverage
par(mfrow = c(1, 1)) # reset plotting layout
# The built-in diagnostic plots are useful but can be improved with ggplot2 for better aesthetics and customization.
gg_lm_diagnostics <- function(fit, label_top = 5) {
aug <- broom::augment(fit) %>%
mutate(
sqrt_abs_std_resid = sqrt(abs(.std.resid)),
obs = row_number()
)
lab <- aug %>% slice_max(.cooksd, n = label_top)
# Consistent theme
theme_slide <- theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold"),
legend.position = "bottom"
)
# 1) Residuals vs Fitted
p1 <- ggplot(aug, aes(.fitted, .resid)) +
geom_point(alpha = 0.25) +
geom_smooth(method = "loess", se = FALSE) +
geom_hline(yintercept = 0, linetype = 2) +
labs(title = "Residuals vs Fitted", x = "Fitted values", y = "Residuals") +
theme_slide
# 2) Normal Q–Q
p2 <- ggplot(aug, aes(sample = .std.resid)) +
stat_qq(alpha = 0.25) +
stat_qq_line() +
labs(title = "Normal Q–Q", x = "Theoretical quantiles", y = "Standardized residuals") +
theme_slide
# 3) Scale–Location
p3 <- ggplot(aug, aes(.fitted, sqrt_abs_std_resid)) +
geom_point(alpha = 0.25) +
geom_smooth(method = "loess", se = FALSE) +
labs(
title = "Scale–Location",
x = "Fitted values",
y = expression(sqrt("|Standardized residuals|"))
) +
theme_slide
# 4) Residuals vs Leverage (+ Cook's contours)
n <- nobs(fit)
p <- length(coef(fit))
cook_levels <- c(0.5, 1)
cook_contour <- function(h, c, p) sqrt(c * p * (1 - h) / h)
h_grid <- seq(max(1e-6, min(aug$.hat, na.rm = TRUE)),
max(aug$.hat, na.rm = TRUE),
length.out = 300)
contour_df <- bind_rows(lapply(cook_levels, function(c) {
data.frame(.hat = h_grid,
y = cook_contour(h_grid, c, p),
level = paste0("Cook's D = ", c))
})) %>%
bind_rows(lapply(cook_levels, function(c) {
data.frame(.hat = h_grid,
y = -cook_contour(h_grid, c, p),
level = paste0("Cook's D = ", c))
}))
p4 <- ggplot(aug, aes(.hat, .std.resid)) +
geom_point(aes(size = .cooksd), alpha = 0.25) +
scale_size_continuous(range = c(1, 6), guide = "none") +
geom_hline(yintercept = 0, linetype = 2) +
geom_vline(xintercept = 2 * p / n, linetype = 3) +
geom_line(
data = contour_df,
aes(.hat, y, linetype = level),
inherit.aes = FALSE
) +
scale_linetype_discrete(name = NULL) +
ggrepel::geom_text_repel(
data = lab,
aes(label = obs),
size = 3,
max.overlaps = Inf
) +
labs(title = "Residuals vs Leverage", x = "Leverage (hat values)", y = "Standardized residuals") +
theme_slide
# Return a 2×2 patchwork object (RStudio will print it automatically)
(p1 | p2) / (p3 | p4)
}
# Plot diagnostics for SLR and MLR
gg_lm_diagnostics(slr_fit)
gg_lm_diagnostics(mlr_fit)
# Plot Residuals vs. lcavol, lweight, svi (partial residuals)
aug <- augment(mlr_fit)
beta_lcavol <- coef(mlr_fit)["lcavol"]
beta_lweight <- coef(mlr_fit)["lweight"]
beta_svi <- coef(mlr_fit)["svi"]
# Partial residuals vs lcavol
aug %>%
mutate(partial_resid_lcavol = .resid + beta_lcavol * lcavol) %>%
ggplot(aes(lcavol, partial_resid_lcavol)) +
geom_point(alpha = 0.25) +
geom_smooth(method = "loess", se = FALSE) +
geom_smooth(method = "lm", se = FALSE, linetype = 2) +
labs(
title = "Partial Residuals vs lcavol (MLR-adjusted)",
x = "lcavol",
y = "Partial residuals"
)
# Partial residuals vs lweight
aug %>%
mutate(partial_resid_lweight = .resid + beta_lweight * lweight) %>%
ggplot(aes(lweight, partial_resid_lweight)) +
geom_point(alpha = 0.25) +
geom_smooth(method = "loess", se = FALSE) +
geom_smooth(method = "lm", se = FALSE, linetype = 2) +
labs(
title = "Partial Residuals vs lweight (MLR-adjusted)",
x = "lweight",
y = "Partial residuals"
)
# Partial residuals vs svi
aug %>%
mutate(partial_resid_svi = .resid + beta_svi * svi) %>%
ggplot(aes(svi, partial_resid_svi)) +
geom_point(alpha = 0.25) +
geom_smooth(method = "lm", se = FALSE, linetype = 2) +
labs(
title = "Partial Residuals vs svi (MLR-adjusted)",
x = "svi",
y = "Partial residuals"
)
# # --- Normality test (Shapiro–Wilk) ---
# # H0: errors are normally distributed
# #shapiro.test(resid(fit1)): will give an error, because it
# #cannot handle more than 5000 residuals.
#
# set.seed(210)
# # For large datasets, sample 5000 residuals for the test
# n <- length(res)
# shapiro.test(sample(resid(mlr_fit), size = min(5000, n)))
#
#
# # --- Homoskedasticity test (Breusch–Pagan) ---
# # H0: Var(eps_i | x_i) = constant (sigma^2)
# bptest(mlr_fit)
#
# # --- Independence / autocorrelation (Durbin–Watson) ---
# # NOTE: DW is meaningful for ordered observations (e.g., time series).
# # Here, recordings come from multiple subjects over time.
# # A simple illustration: sort within subject by test_time.
# pd_ord <- pd %>%
#   arrange(subject., test_time)
#
# fit1_ord <- lm(total_UPDRS ~ PPE, data = pd_ord)
# dwtest(fit1_ord)
#
# # --- Addressing non-normality: Box–Cox transformation ---
# # Requires strictly positive response; total_UPDRS is nonnegative in practice.
# min(pd$total_UPDRS)
#
# # Estimate lambda by maximizing the profile log-likelihood
# bc <- boxcox(fit1, lambda = seq(-2, 2, by = 0.05), plotit = TRUE)
# lambda_hat <- bc$x[which.max(bc$y)]
# lambda_hat
#
# # Transform the response using Box–Cox definition
# boxcox_transform <- function(y, lambda) {
#   if (abs(lambda) < 1e-8) return(log(y))
#   (y^lambda - 1) / lambda
# }
#
# # Apply transformation
# pd$Y_bc <- boxcox_transform(pd$total_UPDRS, lambda_hat)
#
# # Fit linear model with transformed response
# fit1_bc <- lm(Y_bc ~ PPE, data = pd)
# summary(fit1_bc)
#
# # Compare diagnostics pre/post transformation
# par(mfrow = c(2, 2)) # set plotting layout to 2x2
# plot(fit1) # original
# plot(fit1_bc) # Box–Cox transformed
# par(mfrow = c(1, 1)) # reset plotting layout
# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)
library(broom)
library(dplyr)
library(ggrepel)
library(patchwork)
library(lmtest)
# Load data
df <- read_csv("prostate.csv")
# Split data into training and test sets
train_df <- df %>% filter(train == TRUE)
test_df  <- df %>% filter(train == FALSE)
# Validate ntrain and ntest
ntrain <- nrow(train_df)
ntest  <- nrow(test_df)
ntrain
ntest
# Separate train/test predictors (X) and response (y)
y_train <- train_df$lpsa
y_test  <- test_df$lpsa
# Filter out non-predictor columns
X_train <- train_df %>%
select(-lpsa, -train, -id)
X_test <- test_df %>%
select(-lpsa, -train, -id)
# Validate
dim(X_train)
length(y_train)
dim(X_test)
length(y_test)
# Simple Linear Regression (SLR) with the lcavol predictor
slr_fit <- lm(lpsa ~ lcavol, data = train_df)
summary(slr_fit)
# Plot SLR results on training set
ggplot(train_df, aes(x = lcavol, y = lpsa)) +
geom_point(alpha = 0.7) +
geom_smooth(
aes(color = "Linear Regression Line"),
method = "lm",
se = TRUE,
level = 0.95
) +
scale_color_manual(values = c("Linear Regression Line" = "blue")) +
labs(
title = "Simple Linear Regression (SLR): lpsa vs lcavol (Training Set)",
x = "lcavol (log cancer volume)",
y = "lpsa (log PSA)",
color = "Legend"
)
# Generate predictions using the SLR model
pred_test_slr <- predict(slr_fit, newdata = test_df)
# Evaluate SLR model performance on test set
rmse_slr <- RMSE(pred_test_slr, test_df$lpsa)
r2_slr   <- R2(pred_test_slr, test_df$lpsa)
rmse_slr #average prediction error in outcome units (lpsa)
r2_slr
# Plot predicted vs actual for SLR model on test set
ref_line <- tibble(
label = "Perfect Prediction",
slope = 1,
intercept = 0
)
tibble(
actual = test_df$lpsa,
predicted = as.numeric(pred_test_slr)
) %>%
ggplot(aes(x = actual, y = predicted)) +
geom_point(alpha = 0.7) +
geom_abline(
data = ref_line,
aes(slope = slope, intercept = intercept, color = label),
linewidth = 1
) +
scale_color_manual(values = c("Perfect Prediction" = "black")) +
labs(
title = "Simple Linear Regression (SLR): Predicted vs Actual lpsa (Test Set)",
x = "Actual lpsa (Log PSA)",
y = "Predicted lpsa (Log PSA)",
color = "Legend"
)
# SLR Conclusion: The SLR model using lcavol as a predictor in the test set shows moderate
# generalization performance on the test set, with RMSE (0.69) and R-squared (0.54) values.
# With a RMSE of 0.69 on lpsa which translates to about a multiplicative factor of 2 in relative PSA error,
# we can capture the order of magnitude to reasonably separate low vs. high PSA, but yet,
# we cannot reliably distinguish between 6 vs. 9 ng/mL PSA values for clinical decision-making.
# While the R2 tells us that lcavol explains a good portion of variability in lpsa, the remaining 46% remains unexplained.
# These results indicate room for improvement with more predictors in Multiple Linear Regression (MLR) model.
# Multiple Linear Regression (MLR) with all predictors except id, train, and lpsa
mlr_fit <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = train_df)
# MLR fit for top 3 predictors lcavol, svi, lweight
mlr_fit_best <- lm(lpsa ~ lcavol + svi + lweight, data = train_df)
# Coefficient estimates, standard errors, t-tests
summary(mlr_fit)$coefficients
# 95% confidence intervals for beta parameters
confint(mlr_fit, level = 0.95)
# Generate predictions using the MLR model
pred_test_mlr <- predict(mlr_fit, newdata = test_df)
pred_test_mlr_best <- predict(mlr_fit_best, newdata = test_df)
# Evaluate MLR model performance on test set
rmse_mlr <- RMSE(pred_test_mlr, test_df$lpsa)
r2_mlr   <- R2(pred_test_mlr, test_df$lpsa)
rmse_mlr
r2_mlr
# Evaluate MLR (best 3 predictors: lcavol, lweight, svi) model performance on test set
rmse_mlr_best <- RMSE(pred_test_mlr_best, test_df$lpsa)
r2_mlr_best   <- R2(pred_test_mlr_best, test_df$lpsa)
rmse_mlr_best
r2_mlr_best
# Plot predicted vs actual for MLR model on test set
tibble(
actual = test_df$lpsa,
predicted = as.numeric(pred_test_mlr)
) %>%
ggplot(aes(x = actual, y = predicted)) +
geom_point(alpha = 0.7) +
geom_abline(
data = ref_line,
aes(slope = slope, intercept = intercept, color = label),
linewidth = 1
) +
scale_color_manual(values = c("Perfect Prediction" = "black")) +
labs(
title = "Multiple Linear Regression (MLR): Predicted vs Actual lpsa (Test Set)",
x = "Actual lpsa (Log PSA)",
y = "Predicted lpsa (Log PSA)",
color = "Legend"
)
# MLR Conclusion: The MLR model using all predictors in the test set shows worse performance compared to the SLR.
# The RMSE increased from 0.6926 to 0.7220, and R-squared decreased from 0.5438 to 0.5052.
# Based on these metrics, the SLR explains more variance in the outcome than MLR and has lower prediction error.
# This suggests that adding more predictors introduced noise and overfitting rather than improving generalization.
# Assumptions of Linear Regression: Normality, homoskedasticity, independence
# Create diagnostic plots using base R plotting
par(mfrow = c(2, 2)) # set plotting layout to 2x2
plot(slr_fit)  # residuals vs fitted, QQ plot, scale-location, leverage
par(mfrow = c(1, 1)) # reset plotting layout
# The built-in diagnostic plots are useful but can be improved with ggplot2 for better aesthetics and customization.
gg_lm_diagnostics <- function(fit, label_top = 5) {
aug <- broom::augment(fit) %>%
mutate(
sqrt_abs_std_resid = sqrt(abs(.std.resid)),
obs = row_number()
)
lab <- aug %>% slice_max(.cooksd, n = label_top)
# Consistent theme
theme_slide <- theme_minimal(base_size = 12) +
theme(
plot.title = element_text(face = "bold"),
legend.position = "bottom"
)
# 1) Residuals vs Fitted
p1 <- ggplot(aug, aes(.fitted, .resid)) +
geom_point(alpha = 0.25) +
geom_smooth(method = "loess", se = FALSE) +
geom_hline(yintercept = 0, linetype = 2) +
labs(title = "Residuals vs Fitted", x = "Fitted values", y = "Residuals") +
theme_slide
# 2) Normal Q–Q
p2 <- ggplot(aug, aes(sample = .std.resid)) +
stat_qq(alpha = 0.25) +
stat_qq_line() +
labs(title = "Normal Q–Q", x = "Theoretical quantiles", y = "Standardized residuals") +
theme_slide
# 3) Scale–Location
p3 <- ggplot(aug, aes(.fitted, sqrt_abs_std_resid)) +
geom_point(alpha = 0.25) +
geom_smooth(method = "loess", se = FALSE) +
labs(
title = "Scale–Location",
x = "Fitted values",
y = expression(sqrt("|Standardized residuals|"))
) +
theme_slide
# 4) Residuals vs Leverage (+ Cook's contours)
n <- nobs(fit)
p <- length(coef(fit))
cook_levels <- c(0.5, 1)
cook_contour <- function(h, c, p) sqrt(c * p * (1 - h) / h)
h_grid <- seq(max(1e-6, min(aug$.hat, na.rm = TRUE)),
max(aug$.hat, na.rm = TRUE),
length.out = 300)
contour_df <- bind_rows(lapply(cook_levels, function(c) {
data.frame(.hat = h_grid,
y = cook_contour(h_grid, c, p),
level = paste0("Cook's D = ", c))
})) %>%
bind_rows(lapply(cook_levels, function(c) {
data.frame(.hat = h_grid,
y = -cook_contour(h_grid, c, p),
level = paste0("Cook's D = ", c))
}))
p4 <- ggplot(aug, aes(.hat, .std.resid)) +
geom_point(aes(size = .cooksd), alpha = 0.25) +
scale_size_continuous(range = c(1, 6), guide = "none") +
geom_hline(yintercept = 0, linetype = 2) +
geom_vline(xintercept = 2 * p / n, linetype = 3) +
geom_line(
data = contour_df,
aes(.hat, y, linetype = level),
inherit.aes = FALSE
) +
scale_linetype_discrete(name = NULL) +
ggrepel::geom_text_repel(
data = lab,
aes(label = obs),
size = 3,
max.overlaps = Inf
) +
labs(title = "Residuals vs Leverage", x = "Leverage (hat values)", y = "Standardized residuals") +
theme_slide
# Return a 2×2 patchwork object (RStudio will print it automatically)
(p1 | p2) / (p3 | p4)
}
# Plot diagnostics for SLR and MLR
gg_lm_diagnostics(slr_fit)
gg_lm_diagnostics(mlr_fit)
# Plot Residuals vs. lcavol, lweight, svi (partial residuals)
aug <- augment(mlr_fit)
beta_lcavol <- coef(mlr_fit)["lcavol"]
beta_lweight <- coef(mlr_fit)["lweight"]
beta_svi <- coef(mlr_fit)["svi"]
# Partial residuals vs lcavol
aug %>%
mutate(partial_resid_lcavol = .resid + beta_lcavol * lcavol) %>%
ggplot(aes(lcavol, partial_resid_lcavol)) +
geom_point(alpha = 0.25) +
geom_smooth(method = "loess", se = FALSE) +
geom_smooth(method = "lm", se = FALSE, linetype = 2) +
labs(
title = "Partial Residuals vs lcavol (MLR-adjusted)",
x = "lcavol",
y = "Partial residuals"
)
# Partial residuals vs lweight
aug %>%
mutate(partial_resid_lweight = .resid + beta_lweight * lweight) %>%
ggplot(aes(lweight, partial_resid_lweight)) +
geom_point(alpha = 0.25) +
geom_smooth(method = "loess", se = FALSE) +
geom_smooth(method = "lm", se = FALSE, linetype = 2) +
labs(
title = "Partial Residuals vs lweight (MLR-adjusted)",
x = "lweight",
y = "Partial residuals"
)
# Partial residuals vs svi
aug %>%
mutate(partial_resid_svi = .resid + beta_svi * svi) %>%
ggplot(aes(svi, partial_resid_svi)) +
geom_point(alpha = 0.25) +
geom_smooth(method = "lm", se = FALSE, linetype = 2) +
labs(
title = "Partial Residuals vs svi (MLR-adjusted)",
x = "svi",
y = "Partial residuals"
)
# # --- Normality test (Shapiro–Wilk) ---
# # H0: errors are normally distributed
# #shapiro.test(resid(fit1)): will give an error, because it
# #cannot handle more than 5000 residuals.
#
# set.seed(210)
# # For large datasets, sample 5000 residuals for the test
# n <- length(res)
# shapiro.test(sample(resid(mlr_fit), size = min(5000, n)))
#
#
# # --- Homoskedasticity test (Breusch–Pagan) ---
# # H0: Var(eps_i | x_i) = constant (sigma^2)
# bptest(mlr_fit)
#
# # --- Independence / autocorrelation (Durbin–Watson) ---
# # NOTE: DW is meaningful for ordered observations (e.g., time series).
# # Here, recordings come from multiple subjects over time.
# # A simple illustration: sort within subject by test_time.
# pd_ord <- pd %>%
#   arrange(subject., test_time)
#
# fit1_ord <- lm(total_UPDRS ~ PPE, data = pd_ord)
# dwtest(fit1_ord)
#
# # --- Addressing non-normality: Box–Cox transformation ---
# # Requires strictly positive response; total_UPDRS is nonnegative in practice.
# min(pd$total_UPDRS)
#
# # Estimate lambda by maximizing the profile log-likelihood
# bc <- boxcox(fit1, lambda = seq(-2, 2, by = 0.05), plotit = TRUE)
# lambda_hat <- bc$x[which.max(bc$y)]
# lambda_hat
#
# # Transform the response using Box–Cox definition
# boxcox_transform <- function(y, lambda) {
#   if (abs(lambda) < 1e-8) return(log(y))
#   (y^lambda - 1) / lambda
# }
#
# # Apply transformation
# pd$Y_bc <- boxcox_transform(pd$total_UPDRS, lambda_hat)
#
# # Fit linear model with transformed response
# fit1_bc <- lm(Y_bc ~ PPE, data = pd)
# summary(fit1_bc)
#
# # Compare diagnostics pre/post transformation
# par(mfrow = c(2, 2)) # set plotting layout to 2x2
# plot(fit1) # original
# plot(fit1_bc) # Box–Cox transformed
# par(mfrow = c(1, 1)) # reset plotting layout
# MLR fit for top 3 predictors lcavol, svi, lweight
mlr_fit_best <- lm(lpsa ~ lcavol + svi + lweight, data = train_df)
pred_test_mlr_best <- predict(mlr_fit_best, newdata = test_df)
# Evaluate MLR (best 3 predictors: lcavol, lweight, svi) model performance on test set
rmse_mlr_best <- RMSE(pred_test_mlr_best, test_df$lpsa)
r2_mlr_best   <- R2(pred_test_mlr_best, test_df$lpsa)
rmse_mlr_best
r2_mlr_best
